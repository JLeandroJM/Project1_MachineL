{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo\n",
    "\n",
    "\n",
    "$h(x_i) = x_{i}^1w_1 + x_{i}^2w_2 + ... +  x^gw_2 + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(X, W):\n",
    "  return np.dot(X,W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "## Regularizacion Ridge\n",
    "\n",
    "\n",
    "$\\mathcal{L} =  ||Y - XW^t||_2^2 + ||W||_2^2 $\n",
    "\n",
    "$\\mathcal{L} = \\frac{1}{2n}\\sum_{i=0}^n (y_i - h(x_i))^2 + \\sum_{i=0}^gw_i^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Error_Ridge(X, W, Y, lamda):\n",
    "  A = np.linalg.norm(Y  - h(X,W))**2/(2*len(Y))\n",
    "  B = np.linalg.norm(W)**2\n",
    "  return  A+(lamda/len(Y))*B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizacion Lasso\n",
    "\n",
    "\n",
    "$\\mathcal{L} =  ||Y - XW^t||_2^2 + ||W||_1 $\n",
    "\n",
    "$\\mathcal{L} = \\frac{1}{2n}\\sum_{i=0}^n (y_i - h(x_i))^2 + \\sum_{i=0}^gw_i^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Error_Lasso(X, W, Y, lamda):\n",
    "    A = np.linalg.norm(Y - h(X,W))**2/(2*len(Y))\n",
    "    B = np.sum(np.abs(W))\n",
    "    return  A + (lamda/len(Y)) * B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo de derivadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivada(X, W, Y,lamda):\n",
    "    dr=np.matmul((Y - h(X,W)),-X)/len(Y)\n",
    "    return  dr + ((2*lamda)/len(Y))*W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actualiación de parámetros\n",
    "\n",
    "Recuerde: $\\frac{\\partial L}{\\partial w}$ representa un vector con todas las derivadas de la función de pérdida con rescto a W.\n",
    "\n",
    "$W  = W - \\alpha*\\frac{\\partial L}{\\partial W} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(W, alpha,dw):\n",
    "  return W - alpha*dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.ma.extras import column_stack\n",
    "def train(X, Y, epochs, alfa,lamda):\n",
    "    W = np.random.random(X.shape[1])\n",
    "    L = Error(X,W,Y,lamda)\n",
    "    loss = []\n",
    "    for i in range(epochs):\n",
    "        dW = derivada(X, W, Y, lamda)\n",
    "        W = update(W,alfa, dW)\n",
    "        L = Error(X, W,Y,lamda)\n",
    "        loss.append(L)\n",
    "        if ((i%1000)==0):\n",
    "          print(\"loss value error :\" + str(L))\n",
    "    return W, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones extras para graficar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemente la función plot para mostrar como cambia la función de pérdida \n",
    "def plot_loss_1(epochs_list, loss_list):\n",
    "  plt.plot(epochs_list, loss_list)\n",
    "  plt.xlabel('epochs')\n",
    "  plt.ylabel('loss')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemente la función plot para mostrar la función aproximada al conjunto de puntos\n",
    "def plot_loss(X_real, Y_real, Y_aprox):\n",
    "  plt.plot(X_real, Y_real, 'b*')\n",
    "  plt.plot(X_real, Y_aprox, 'r-')\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('y')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizacion\n",
    "\n",
    "def normalizar(x_ds):\n",
    "  ones_column = np.ones((len(x_ds), 1))  \n",
    "  X = np.array(x_ds).reshape((-1, 1))\n",
    "  X = np.column_stack((ones_column, X))\n",
    "  columns = np.ones((len(x_ds), len(x_ds)-1))  \n",
    "  X = np.column_stack((X,columns))\n",
    "  for j in range(len(X)):\n",
    "    for i in range(2, len(x_ds)+1):\n",
    "      X[j][i] = X[j][1]**i\n",
    "  \n",
    "  return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = normalizar(x_ds)\n",
    "y = y_ds\n",
    "epochs = 10000\n",
    "alfa = 0.1\n",
    "lamda = 0.01\n",
    "W = train(x, y, epochs, alfa, lamda)\n",
    "plot_loss(x_ds, y_ds, h(x,W[0]))\n",
    "plot_loss_1(range(40000), W[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
